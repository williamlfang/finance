if(length(ref)!=0){
for(i in 1: length(ref)){
main_remaining[i]  <- ref[i] %>%
read_html()  %>%
html_nodes(".content p") %>%
html_text() %>%
#.[-c((length(.)-2) : length(.) )] %>%
paste(., collapse = '\n\n')
}}
mainx <- c(main, main_remaining)
all <- list(link, title, date, mainx, ref)   ## all information: link, title, date, main
#return(all)
}
inf <- lapply(link[1], article)
inf
inf[[1]][[2]]
inf[[1]][[3]]
inf[[1]][[5]]
inf[[1]][[4]]
str(inf[[1]][[4]])
mainx
str(mainx)
main_remaining
link
x = link[1]
main <- x %>%
read_html()  %>%
html_nodes(".content p") %>%
html_text() %>%
#.[-c((length(.)-2) : length(.) )] %>%
paste(., collapse = '\n\n')
main_remaining <- rep(NA, length(ref))
if(length(ref)!=0){
for(i in 1: length(ref)){
main_remaining[i]  <- ref[i] %>%
read_html()  %>%
html_nodes(".content p") %>%
html_text() %>%
#.[-c((length(.)-2) : length(.) )] %>%
paste(., collapse = '\n\n')
}}
str(main_remaining)
length(ref)
ref  <- x %>%
read_html() %>%
html_nodes("p  a") %>%
html_attr("href")  %>%
.[grep("2015", .)]  %>%
.[grep("page", .)]
ref
main_remaining <- rep(NA, length(ref))
if(length(ref)!=0){
for(i in 1: length(ref)){
main_remaining[i]  <- ref[i] %>%
read_html()  %>%
html_nodes(".content p") %>%
html_text() %>%
#.[-c((length(.)-2) : length(.) )] %>%
paste(., collapse = '\n\n')
}}
main_remaining
str(main_remaining)
main <- x %>%
read_html()  %>%
html_nodes(".content p") %>%
html_text() %>%
#.[-c((length(.)-2) : length(.) )] %>%
paste(., collapse = '\n\n')
str(main)
main_remaining <- rep(NA, length(ref))
main_remainingx <- rep(NA)
if(length(ref)!=0){
for(i in 1: length(ref)){
main_remaining[i]  <- ref[i] %>%
read_html()  %>%
html_nodes(".content p") %>%
html_text() %>%
#.[-c((length(.)-2) : length(.) )] %>%
paste(., collapse = '\n\n')
main_remainingx <- paste(main_remainingx, main_remaining[i])
}}
main_remainingx
str()
str(main_remainingx)
BrowserUrl(x)
help(browse)
??browse
BROWSE("http://google.com")
browser(x)
x
library(rvest)
url <- "http://stock.stockstar.com/list/sectors.htm"
link <- url %>%
read_html(encoding="GB18030") %>%
html_nodes(".listtable  li  a") %>%
html_attr("href")
url <- "http://stock.stockstar.com/list/sectors.htm"
link <- url %>%
read_html(encoding="GB18030") %>%
html_nodes(".listtable  li  a") %>%
html_attr("href")
link
x=link[1]
x
title  <- x %>%
read_html(encoding="GB18030") %>%
html_nodes("h1") %>%
html_text()
title
page.numbers <- x %>%
read_html(encoding="GB18030")  %>%
html_nodes("#Page a") %>%
html_text() %>%
as.numeric()
page.numbers <- page.numbers[1:length(page.numbers)-1]
page.numbers
date <- x %>%
read_html(encoding="GB18030") %>%
html_nodes("#pubtime_baidu") %>%
html_text()
date <- x %>%
read_html(encoding="GB18030") %>%
html_nodes(".source #pubtime_baidu") %>%
html_text()
date <- x %>%
read_html() %>%
html_nodes(".source #pubtime_baidu") %>%
html_text()
date <- x %>%
read_html(encoding="GB18030") %>%
html_nodes(".source #pubtime_baidu") %>%
html_text()
date
url0 <- strsplit(x, "\\.shtml")
url.new <- rep( NA, length(page.numbers) )
for ( i in 1:length(page.numbers) ){
url.new[i] <- paste(url0, "_", i, ".shtml", sep="")
}
text <- function(x){
text <- x %>%
read_html(encoding="GB18030")  %>%
html_nodes("#container-article  p") %>%
html_text()
text <- text[3:length(text)-1]
}
main <- lapply(url.new, text)
url.new
lapply(url.new[1],text)
lapply(url.new,text)
lapply(url.new[2],text)
lapply(url.new[3],text)
lapply(url.new[4],text)
lapply(url.new[5],text)
main <- apply(url.new, text)
main <- apply(url.new, text)
lapply(url.new[5],text)
main <- sapply(url.new, text)
main <- lapply(url.new[1], text)
main
url.new
length(url.new)
main <- rep(NA)
main <- rep(NA)
for(i in 1:length(url.new)){
main[1] <- text(url.new[i])
}
i=1
main[1] <- text(url.new[i])
main <- rep(NA)
for(i in 1:length(url.new)){
main[i] <- text(url.new[i])
}
for(i in 1:length(url.new)){
main[i] <- text(url.new[i])
}
main
i=1
i=2
main[i] <- text(url.new[i])
main
main <- rep(NA)
main
main[i] <- text(url.new[i])
main <- list()
main[i] <- text(url.new[i])
main
main <- list()
for(i in 1:length(url.new)){
main[[i]] <- text(url.new[i])
}
main[[i]] <- text(url.new[i])
main
for(i in 1:length(url.new)){
main[[i]] <- text(url.new[i])
}
i=3
main[[i]] <- text(url.new[i])
text(url.new[3])
m = text(url.new[3])
url.new[3]
text(m)
m
y = url.new[3]
m text(y)
m = text(y)
x=url.new[3]
text <- x %>%
read_html(encoding="GB18030")  %>%
html_nodes("#container-article  p") %>%
html_text()
text
text <- text[3:length(text)-1]
text
text(url.new[3])
main <- lapply(url.new, text)
library(rvest)
url <- "http://stock.stockstar.com/list/sectors.htm"
link <- url %>%
read_html(encoding="GB18030") %>%
html_nodes(".listtable  li  a") %>%
html_attr("href")
link[2]
x = link[2]
browseURL(x)
article <- function(x){
link <- x
title  <- x %>%
read_html(encoding="GB18030") %>%
html_nodes("h1") %>%
html_text()
page.numbers <- x %>%
read_html(encoding="GB18030")  %>%
html_nodes("#Page a") %>%
html_text() %>%
as.numeric()
page.numbers <- page.numbers[1:length(page.numbers)-1]
date <- x %>%
read_html(encoding="GB18030") %>%
html_nodes("#pubtime_baidu") %>%
html_text()
url0 <- strsplit(x, "\\.shtml")
url.new <- rep( NA, length(page.numbers) )
for ( i in 1:length(page.numbers) ){
url.new[i] <- paste(url0, "_", i, ".shtml", sep="")
}
text <- function(x){
text <- x %>%
read_html(encoding="GB18030")  %>%
html_nodes("#container-article  p") %>%
html_text()
text <- text[3:length(text)-1]
}
main <- lapply(url.new, text)
tidai <- function(x){
# y = paste(x, collapse = '\t ============================================================================ \t')
y = paste(x, collapse = '\n                                                                                 \n')
return(y)
}
main <- main %>%
sapply(., tidai) %>%
as.character() %>%
gsub("个股资料|操作策略|咨询高手|实盘买卖", "", .) %>%
gsub("(\\（|\\）)"," ..........................................................", .)
all <- list(link, title, date, main)   ## all information: link, title, date, main
return(all)
}
inf <- article(link[2])
inf
source('~/finance/_posts/资讯/_Drafts/inf-散户之家.R', echo=TRUE)
url <- "http://stock.stockstar.com/list/sectors.htm"
link <- url %>%
read_html(encoding="GB18030") %>%
html_nodes(".listtable  li  a") %>%
html_attr("href")
x=link[3]
browseURL(x)
browseURL(x)
article <- function(x){
link <- x
title  <- x %>%
read_html(encoding="GB18030") %>%
html_nodes("h1") %>%
html_text()
page.numbers <- x %>%
read_html(encoding="GB18030")  %>%
html_nodes("#Page a") %>%
html_text() %>%
as.numeric()
page.numbers <- page.numbers[1:length(page.numbers)-1]
date <- x %>%
read_html(encoding="GB18030") %>%
html_nodes("#pubtime_baidu") %>%
html_text()
url0 <- strsplit(x, "\\.shtml")
url.new <- rep( NA, length(page.numbers) )
for ( i in 1:length(page.numbers) ){
url.new[i] <- paste(url0, "_", i, ".shtml", sep="")
}
text <- function(x){
text <- x %>%
read_html(encoding="GB18030")  %>%
html_nodes("#container-article  p") %>%
html_text()
text <- text[3:length(text)-1]
}
main <- lapply(url.new, text)
tidai <- function(x){
# y = paste(x, collapse = '\t ============================================================================ \t')
y = paste(x, collapse = '\n                                                                                 \n')
return(y)
}
main <- main %>%
sapply(., tidai) %>%
as.character() %>%
gsub("个股资料|操作策略|咨询高手|实盘买卖", "", .) %>%
gsub("(\\（|\\）)"," ..........................................................", .)
all <- list(link, title, date, main)   ## all information: link, title, date, main
return(all)
}
article(x)
m = article(x)
m[[4]]
link
source('~/finance/_posts/资讯/_Drafts/inf-板块掘金.R', echo=TRUE)
rm(list=ls(all=TRUE))
require(knitr, quietly=TRUE, warn.conflicts=FALSE)
require(rmarkdown)
library(rvest)
setwd("~/finance/_posts/资讯/_Drafts")
dir=getwd()
images.dir=dir
images.url='/finance/assets/images/'
out_ext='.md'
in_ext='.Rmd'
recursive=TRUE
f <- "板块掘金.Rmd"
message(paste("Processing ", f, sep=''))
content <- readLines(f)
frontMatter <- which(substr(content, 1, 3) == '---')
if(length(frontMatter) >= 2 & 1 %in% frontMatter) {
statusLine <- which(substr(content, 1, 7) == 'status:')
publishedLine <- which(substr(content, 1, 10) == 'published:')
if(statusLine > frontMatter[1] & statusLine < frontMatter[2]) {
status <- unlist(strsplit(content[statusLine], ':'))[2]
status <- sub('[[:space:]]+$', '', status)
status <- sub('^[[:space:]]+', '', status)
if(tolower(status) == 'process') {
#This is a bit of a hack but if a line has zero length (i.e. a
#black line), it will be removed in the resulting markdown file.
#This will ensure that all line returns are retained.
content[nchar(content) == 0] <- ' '
message(paste('Processing ', f, sep=''))
content[statusLine] <- 'status: publish'
content[publishedLine] <- 'published: true'
outFile <- paste(substr(f, 1, (nchar(f)-(nchar(in_ext)))), out_ext, sep='')
render_markdown(strict=TRUE)
opts_knit$set(out.format='md')
opts_knit$set(base.dir=images.dir)
opts_knit$set(base.url=images.url)
######################################################################
## 产生的图片存储位置 `/assets/images/r-figures/`
#fig.path <- paste0("r-figures/", sub(".Rmd$", "", basename(files)), "/")
#opts_chunk$set(fig.path = fig.path)
## opts_chunk$set(fig.cap = "center")  ## figure position
## render_jekyll()
######################################################################
try(knit(text=content, output=outFile), silent=FALSE)
} else {
warning(paste("Not processing ", f, ", status is '", status,
"'. Set status to 'process' to convert.", sep=''))
}
} else {
warning("Status not found in front matter.")
}
} else {
warning("No front matter found. Will not process this file.")
}
source('~/finance/_posts/资讯/_Drafts/inf-板块掘金.R', echo=TRUE)
source('~/finance/_posts/资讯/_Drafts/inf-板块掘金.R', echo=TRUE)
rm(list=ls(all=TRUE))
require(knitr, quietly=TRUE, warn.conflicts=FALSE)
require(rmarkdown)
library(rvest)
setwd("~/finance/_posts/资讯/_Drafts")
dir=getwd()
images.dir=dir
images.url='/finance/assets/images/'
out_ext='.md'
in_ext='.Rmd'
recursive=TRUE
#############################################################################
f <- "板块掘金.Rmd"
message(paste("Processing ", f, sep=''))
content <- readLines(f)
frontMatter <- which(substr(content, 1, 3) == '---')
if(length(frontMatter) >= 2 & 1 %in% frontMatter) {
statusLine <- which(substr(content, 1, 7) == 'status:')
publishedLine <- which(substr(content, 1, 10) == 'published:')
if(statusLine > frontMatter[1] & statusLine < frontMatter[2]) {
status <- unlist(strsplit(content[statusLine], ':'))[2]
status <- sub('[[:space:]]+$', '', status)
status <- sub('^[[:space:]]+', '', status)
if(tolower(status) == 'process') {
#This is a bit of a hack but if a line has zero length (i.e. a
#black line), it will be removed in the resulting markdown file.
#This will ensure that all line returns are retained.
content[nchar(content) == 0] <- ' '
message(paste('Processing ', f, sep=''))
content[statusLine] <- 'status: publish'
content[publishedLine] <- 'published: true'
outFile <- paste(substr(f, 1, (nchar(f)-(nchar(in_ext)))), out_ext, sep='')
render_markdown(strict=TRUE)
opts_knit$set(out.format='md')
opts_knit$set(base.dir=images.dir)
opts_knit$set(base.url=images.url)
######################################################################
## 产生的图片存储位置 `/assets/images/r-figures/`
#fig.path <- paste0("r-figures/", sub(".Rmd$", "", basename(files)), "/")
#opts_chunk$set(fig.path = fig.path)
## opts_chunk$set(fig.cap = "center")  ## figure position
## render_jekyll()
######################################################################
try(knit(text=content, output=outFile), silent=FALSE)
} else {
warning(paste("Not processing ", f, ", status is '", status,
"'. Set status to 'process' to convert.", sep=''))
}
} else {
warning("Status not found in front matter.")
}
} else {
warning("No front matter found. Will not process this file.")
}
if(length(frontMatter) >= 2 & 1 %in% frontMatter) {
statusLine <- which(substr(content, 1, 7) == 'status:')
publishedLine <- which(substr(content, 1, 10) == 'published:')
if(statusLine > frontMatter[1] & statusLine < frontMatter[2]) {
status <- unlist(strsplit(content[statusLine], ':'))[2]
status <- sub('[[:space:]]+$', '', status)
status <- sub('^[[:space:]]+', '', status)
if(tolower(status) == 'process') {
#This is a bit of a hack but if a line has zero length (i.e. a
#black line), it will be removed in the resulting markdown file.
#This will ensure that all line returns are retained.
content[nchar(content) == 0] <- ' '
message(paste('Processing ', f, sep=''))
content[statusLine] <- 'status: publish'
content[publishedLine] <- 'published: true'
outFile <- paste(substr(f, 1, (nchar(f)-(nchar(in_ext)))), out_ext, sep='')
render_markdown(strict=TRUE)
opts_knit$set(out.format='md')
opts_knit$set(base.dir=images.dir)
opts_knit$set(base.url=images.url)
######################################################################
## 产生的图片存储位置 `/assets/images/r-figures/`
#fig.path <- paste0("r-figures/", sub(".Rmd$", "", basename(files)), "/")
#opts_chunk$set(fig.path = fig.path)
## opts_chunk$set(fig.cap = "center")  ## figure position
## render_jekyll()
######################################################################
try(knit(text=content, output=outFile), silent=FALSE)
} else {
warning(paste("Not processing ", f, ", status is '", status,
"'. Set status to 'process' to convert.", sep=''))
}
} else {
warning("Status not found in front matter.")
}
} else {
warning("No front matter found. Will not process this file.")
}
if(length(frontMatter) >= 2 & 1 %in% frontMatter) {
statusLine <- which(substr(content, 1, 7) == 'status:')
publishedLine <- which(substr(content, 1, 10) == 'published:')
if(statusLine > frontMatter[1] & statusLine < frontMatter[2]) {
status <- unlist(strsplit(content[statusLine], ':'))[2]
status <- sub('[[:space:]]+$', '', status)
status <- sub('^[[:space:]]+', '', status)
if(tolower(status) == 'process') {
#This is a bit of a hack but if a line has zero length (i.e. a
#black line), it will be removed in the resulting markdown file.
#This will ensure that all line returns are retained.
content[nchar(content) == 0] <- ' '
message(paste('Processing ', f, sep=''))
content[statusLine] <- 'status: publish'
content[publishedLine] <- 'published: true'
outFile <- paste(substr(f, 1, (nchar(f)-(nchar(in_ext)))), out_ext, sep='')
render_markdown(strict=TRUE)
opts_knit$set(out.format='md')
opts_knit$set(base.dir=images.dir)
opts_knit$set(base.url=images.url)
######################################################################
## 产生的图片存储位置 `/assets/images/r-figures/`
#fig.path <- paste0("r-figures/", sub(".Rmd$", "", basename(files)), "/")
#opts_chunk$set(fig.path = fig.path)
## opts_chunk$set(fig.cap = "center")  ## figure position
## render_jekyll()
######################################################################
try(knit(text=content, output=outFile), silent=FALSE)
} else {
warning(paste("Not processing ", f, ", status is '", status,
"'. Set status to 'process' to convert.", sep=''))
}
} else {
warning("Status not found in front matter.")
}
} else {
warning("No front matter found. Will not process this file.")
}
source('~/finance/_posts/资讯/_Drafts/inf-板块掘金.R', echo=TRUE)
source('~/finance/_posts/资讯/_Drafts/inf-板块掘金.R', echo=TRUE)
source('~/finance/_posts/资讯/_Drafts/inf.R', echo=TRUE)
