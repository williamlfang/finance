read_html(encoding="GB18030") %>%
html_nodes(".listtable  li  a") %>%
html_attr("href")
## link <- c(link1, link2, link3)
link <- c(link1, link2, link3) %>% .[-item2Remove]
date1 <- url1 %>%
read_html(encoding="GB18030") %>%
html_nodes(".listnews   span") %>%
html_text() %>%
substr(., 1, 10)
date2 <- url2 %>%
read_html(encoding="GB18030") %>%
html_nodes(".listnews   span") %>%
html_text() %>%
substr(., 1, 10)
date3 <- url3 %>%
read_html(encoding="GB18030") %>%
html_nodes(".listnews   span") %>%
html_text() %>%
substr(., 1, 10)
#date <- c(date1, date2, date3)
date <- c(date1, date2, date3) %>% .[-item2Remove]
article <- function(x){
title  <- x %>%
read_html(encoding="GB18030") %>%
html_nodes("h1") %>%
html_text()
date <- x %>%
read_html(encoding="GB18030") %>%
html_nodes("#pubtime_baidu") %>%
html_text() %>%
substr(., 1, 10)
file.link <-  x %>%
read_html(encoding="GB18030") %>%
html_nodes("#container-article  p  a") %>%
html_attr("href") %>%
.[1]
all <- list(title, date, file.link)   ## all information: link, title, date, main
return(all)
}
x =link[1]
title  <- x %>%
read_html(encoding="GB18030") %>%
html_nodes("h1") %>%
html_text()
title  <- x %>%
read_html(encoding="GB18030") %>%
html_nodes("h1") %>%
html_text()
date <- x %>%
read_html(encoding="GB18030") %>%
html_nodes("#pubtime_baidu") %>%
html_text() %>%
substr(., 1, 10)
file.link <-  x %>%
read_html(encoding="GB18030") %>%
html_nodes("#container-article  p  a") %>%
html_attr("href") %>%
.[1]
all <- list(title, date, file.link)   ## all information: link, title, date, main
return(all)
all
title  <- x %>%
read_html(encoding="GB18030") %>%
html_nodes("h1") %>%
html_text()
date <- x %>%
read_html(encoding="GB18030") %>%
html_nodes("#pubtime_baidu") %>%
html_text() %>%
substr(., 1, 10)
file.link <-  x %>%
read_html(encoding="GB18030") %>%
html_nodes("#container-article  p  a") %>%
html_attr("href") %>%
.[1]
all <- list(title, date, file.link)   ## all information: link, title, date, main
all
inf = lapply(link[which(date==Date)[1]:(which(date==Date)[1]+length(which(date==Date))-1)], tryCatch(article) )
link[which(date==Date)[1]:(which(date==Date)[1]+length(which(date==Date))-1)]
library(rvest)
library(downloader)
library(beepr)
setwd("/media/william/Storage/国信证券/Reports/公司研报")
Date <- Sys.Date()-1
file <- Date %>%
as.character()
if (file.exists(file)){
setwd(file.path(file))
} else {
dir.create(file.path(file))
setwd(file.path(file))
}
## download function
dl <- function(x){
destfile <- paste(x[[1]], "-", x[[2]], ".pdf", sep="")
try(download(x[[3]], destfile))
}
## 证券之星·公司研究
url1 <- "http://stock.stockstar.com/list/3491_1.shtml"
url2 <- "http://stock.stockstar.com/list/3491_2.shtml"
url3 <- "http://stock.stockstar.com/list/3491_3.shtml"
title1 <- url1 %>%
read_html(encoding="GB18030") %>%
html_nodes(".listtable  li  a") %>%
html_text()
title2 <- url2 %>%
read_html(encoding="GB18030") %>%
html_nodes(".listtable  li  a") %>%
html_text()
title3 <- url3 %>%
read_html(encoding="GB18030") %>%
html_nodes(".listtable  li  a") %>%
html_text()
title2Remove <- c(title1, title2, title3)
item2Remove <- c(grep("[a-zA-Z]", title2Remove))
title <- title2Remove[-item2Remove]
link1 <- url1 %>%
read_html(encoding="GB18030") %>%
html_nodes(".listtable  li  a") %>%
html_attr("href")
link2 <- url2 %>%
read_html(encoding="GB18030") %>%
html_nodes(".listtable  li  a") %>%
html_attr("href")
link3 <- url3 %>%
read_html(encoding="GB18030") %>%
html_nodes(".listtable  li  a") %>%
html_attr("href")
## link <- c(link1, link2, link3)
link <- c(link1, link2, link3) %>% .[-item2Remove]
date1 <- url1 %>%
read_html(encoding="GB18030") %>%
html_nodes(".listnews   span") %>%
html_text() %>%
substr(., 1, 10)
date2 <- url2 %>%
read_html(encoding="GB18030") %>%
html_nodes(".listnews   span") %>%
html_text() %>%
substr(., 1, 10)
date3 <- url3 %>%
read_html(encoding="GB18030") %>%
html_nodes(".listnews   span") %>%
html_text() %>%
substr(., 1, 10)
#date <- c(date1, date2, date3)
date <- c(date1, date2, date3) %>% .[-item2Remove]
date
link[which(date==Date)[1]:(which(date==Date)[1]+length(which(date==Date))-1)]
inf = lapply(link[which(date==Date)[1]:(which(date==Date)[1]+length(which(date==Date))-1)], article)
inf = lapply(link[1], article)
inf
lapply(inf, dl)
link[which(date==Date)[1]:(which(date==Date)[1]+length(which(date==Date))-1)]
inf = lapply(link[which(date==Date)[1]:(which(date==Date)[1]+length(which(date==Date))-1)], article)
lapply(inf, dl)
###############################################################################################
setwd("/media/william/Storage/国信证券/Reports/公司研报")
Date <- Sys.Date()-1
file <- Date %>%
as.character()
if (file.exists(file)){
setwd(file.path(file))
} else {
dir.create(file.path(file))
setwd(file.path(file))
}
## download function
dl <- function(x){
destfile <- paste(x[[1]][[1]], "-", x[[2]][1], ".pdf", sep="")
try(download(x[[3]][1], destfile))
}
##
url1 <- "http://bg.panlv.net/2-1-1.html"
url2 <- "http://bg.panlv.net/2-1-2.html"
url3 <- "http://bg.panlv.net/2-1-3.html"
url4 <- "http://bg.panlv.net/2-1-4.html"
link1 <- url1 %>%
read_html(encoding="GB18030") %>%
html_nodes(".divborder  th  a") %>%
html_attr("href")
link2 <- url2 %>%
read_html(encoding="GB18030") %>%
html_nodes(".divborder  th  a") %>%
html_attr("href")
link3 <- url3 %>%
read_html(encoding="GB18030") %>%
html_nodes(".divborder  th  a") %>%
html_attr("href")
link4 <- url4 %>%
read_html(encoding="GB18030") %>%
html_nodes(".divborder  th  a") %>%
html_attr("href")
link <- c(link1, link2, link3, link4)
date1 <- url1 %>%
read_html(encoding="GB18030") %>%
html_nodes(".divborder  td") %>%
html_text()
date2 <- url2 %>%
read_html(encoding="GB18030") %>%
html_nodes(".divborder  td") %>%
html_text()
date3 <- url3 %>%
read_html(encoding="GB18030") %>%
html_nodes(".divborder  td") %>%
html_text()
date4 <- url4 %>%
read_html(encoding="GB18030") %>%
html_nodes(".divborder  td") %>%
html_text()
date <- c(date1, date2, date3, date4) %>% grep("^2015",., value=TRUE)
article <- function(x){
title  <- x %>%
read_html(encoding="GB18030") %>%
html_nodes("h1") %>%
html_text() %>%
.[1]
date <- x %>%
read_html(encoding="GB18030") %>%
html_nodes(".linet") %>%
html_text() %>%
.[3] %>%
gsub("发布日期：", "", .)  %>%
as.Date()
file.link <-  x %>%
read_html(encoding="GB18030") %>%
html_nodes("table   td   .maindown4") %>%
html_attr("href") %>%
paste0("http://bg.panlv.net/",.)
all <- list(title, date, file.link)   ## all information: link, title, date, main
return(all)
}
inf = lapply(link[which(date==Date)[1]:(which(date==Date)[1]+length(which(date==Date))-1)], article)
inf = lapply(link[which(date==Date)[1]:(which(date==Date)[1]+length(which(date==Date))-1)], article)
inf = lapply(link[1:50], try(article))
inf = lapply(link[1:2], try(article))
lapply(inf, dl)
inf = lapply(link[3:20], try(article))
inf = lapply(link[3:10], try(article))
inf = lapply(link[3:5], try(article))
inf = lapply(link[4:5], try(article))
inf = lapply(link[5], try(article))
inf = lapply(link[6], try(article))
link
source('/media/william/Storage/国信证券/Reports/东方财富网/ind.R', echo=TRUE)
source('/media/william/Storage/国信证券/Reports/东方财富网/ind.R', echo=TRUE)
source('/media/william/Storage/国信证券/Reports/东方财富网/ind.R', echo=TRUE)
source('~/finance/_posts/行业/_Drafts/industry.R', echo=TRUE)
source('~/finance/_posts/行业/_Drafts/converting_function.R', echo=TRUE)
setwd("~/finance/_posts/行业/_Drafts")
dir=getwd()
images.dir=dir
images.url='/investment/assets/images/'
out_ext='.md'
in_ext='.rmd'
recursive=FALSE
files <- list.files(path=dir, pattern=in_ext, ignore.case=TRUE, recursive=recursive)
f = files[length(files)]                  ################ wich files ##############
message(paste("Processing ", f, sep=''))
content <- readLines(f)
frontMatter <- which(substr(content, 1, 3) == '---')
if(length(frontMatter) >= 2 & 1 %in% frontMatter) {
statusLine <- which(substr(content, 1, 7) == 'status:')
publishedLine <- which(substr(content, 1, 10) == 'published:')
if(statusLine > frontMatter[1] & statusLine < frontMatter[2]) {
status <- unlist(strsplit(content[statusLine], ':'))[2]
status <- sub('[[:space:]]+$', '', status)
status <- sub('^[[:space:]]+', '', status)
if(tolower(status) == 'process') {
#This is a bit of a hack but if a line has zero length (i.e. a
#black line), it will be removed in the resulting markdown file.
#This will ensure that all line returns are retained.
content[nchar(content) == 0] <- ' '
message(paste('Processing ', f, sep=''))
content[statusLine] <- 'status: publish'
content[publishedLine] <- 'published: true'
outFile <- paste(substr(f, 1, (nchar(f)-(nchar(in_ext)))), out_ext, sep='')
render_markdown(strict=TRUE)
opts_knit$set(out.format='md')
opts_knit$set(base.dir=images.dir)
opts_knit$set(base.url=images.url)
######################################################################
## 产生的图片存储位置 `/assets/images/r-figures/`
fig.path <- paste0("r-figures/", sub(".Rmd$", "", basename(files)), "/")
opts_chunk$set(fig.path = fig.path)
## opts_chunk$set(fig.cap = "center")  ## figure position
## render_jekyll()
######################################################################
try(knit(text=content, output=outFile), silent=FALSE)
} else {
warning(paste("Not processing ", f, ", status is '", status,
"'. Set status to 'process' to convert.", sep=''))
}
} else {
warning("Status not found in front matter.")
}
} else {
warning("No front matter found. Will not process this file.")
}
url1 <- "http://stock.stockstar.com/list/3489_1.shtml"
url2 <- "http://stock.stockstar.com/list/3489_2.shtml"
url3 <- "http://stock.stockstar.com/list/3489_3.shtml"
link1 <- url1 %>%
read_html(encoding="GB18030") %>%
html_nodes(".listtable  li  a") %>%
html_attr("href")
link2 <- url2 %>%
read_html(encoding="GB18030") %>%
html_nodes(".listtable  li  a") %>%
html_attr("href")
link3 <- url3 %>%
read_html(encoding="GB18030") %>%
html_nodes(".listtable  li  a") %>%
html_attr("href")
link <- c(link1, link2, link3)
article <- function(x){
link <- x
title  <- x %>%
read_html(encoding="GB18030") %>%
html_nodes("h1") %>%
html_text()
date <- x %>%
read_html(encoding="GB18030") %>%
html_nodes("#pubtime_baidu") %>%
html_text() %>%
substr(., 1, 10)
main <- x %>%
read_html(encoding="GB18030")  %>%
html_nodes("#container-article  p") %>%
html_text() %>%
gsub("个股资料|操作策略|咨询高手|实盘买卖|相关附件", "", .) %>%
gsub("(\\（|\\）)"," ..........................................................", .) %>%
paste(., collapse = '\n                                                                                              \n')
file <-  x %>%
read_html(encoding="GB18030") %>%
html_nodes("#container-article  p  a") %>%
html_attr("href") %>%
.[1]
file.link <-  x %>%
read_html(encoding="GB18030") %>%
html_nodes("#container-article   p   a") %>%
html_text() %>%
.[1]
all <- list(link, title, date, main, file, file.link)   ## all information: link, title, date, main
return(all)
}
inf = lapply(link, article)
inf
rm(list=ls())
library(rvest)
rm(list=ls())
url1 <- "http://stock.stockstar.com/list/3489_1.shtml"
url2 <- "http://stock.stockstar.com/list/3489_2.shtml"
url3 <- "http://stock.stockstar.com/list/3489_3.shtml"
link1 <- url1 %>%
read_html(encoding="GB18030") %>%
html_nodes(".listtable  li  a") %>%
html_attr("href")
link2 <- url2 %>%
read_html(encoding="GB18030") %>%
html_nodes(".listtable  li  a") %>%
html_attr("href")
link3 <- url3 %>%
read_html(encoding="GB18030") %>%
html_nodes(".listtable  li  a") %>%
html_attr("href")
link <- c(link1, link2, link3)
article <- function(x){
link <- x
title  <- x %>%
read_html(encoding="GB18030") %>%
html_nodes("h1") %>%
html_text()
date <- x %>%
read_html(encoding="GB18030") %>%
html_nodes("#pubtime_baidu") %>%
html_text() %>%
substr(., 1, 10)
main <- x %>%
read_html(encoding="GB18030")  %>%
html_nodes("#container-article  p") %>%
html_text() %>%
gsub("个股资料|操作策略|咨询高手|实盘买卖|相关附件", "", .) %>%
gsub("(\\（|\\）)"," ..........................................................", .) %>%
paste(., collapse = '\n                                                                                              \n')
file <-  x %>%
read_html(encoding="GB18030") %>%
html_nodes("#container-article  p  a") %>%
html_attr("href") %>%
.[1]
file.link <-  x %>%
read_html(encoding="GB18030") %>%
html_nodes("#container-article   p   a") %>%
html_text() %>%
.[1]
all <- list(link, title, date, main, file, file.link)   ## all information: link, title, date, main
return(all)
}
inf = lapply(link, article)
除此之外，企业申请新三板挂牌转让的时间周期还依赖于企业确定相关中介机构、相关中介机构进行尽职调查以及获得协会确认函后的后续事宜安排。
source('~/finance/_posts/行业/_Drafts/industry.R', echo=TRUE)
[`r inf[[14]][[6]]`](`r inf[[14]][[5]]`)
source('~/finance/_posts/行业/_Drafts/industry (copy).R', echo=TRUE)
library(rvest)
url <- "http://stock.stockstar.com/list/sectors.htm"
browseURL(url)
link <- url %>%
read_html(encoding="GB18030") %>%
html_nodes(".listtable  li  a") %>%
html_attr("href")
link
article <- function(x){
}
article <- function(x){
link <- x
title  <- x %>%
read_html(encoding="GB18030") %>%
html_nodes("h1") %>%
html_text()
page.numbers <- x %>%
read_html(encoding="GB18030")  %>%
html_nodes("#Page a") %>%
html_text() %>%
as.numeric()
page.numbers <- page.numbers[1:length(page.numbers)-1]
date <- x %>%
read_html(encoding="GB18030") %>%
html_nodes("#pubtime_baidu") %>%
html_text()
url0 <- strsplit(x, "\\.shtml")
url.new <- rep( NA, length(page.numbers) )
for ( i in 1:length(page.numbers) ){
url.new[i] <- paste(url0, "_", i, ".shtml", sep="")
}
text <- function(x){
text <- x %>%
read_html(encoding="GB18030")  %>%
html_nodes("#container-article  p") %>%
html_text()
text <- text[3:length(text)-1]
}
main <- lapply(url.new, text)
tidai <- function(x){
# y = paste(x, collapse = '\t ============================================================================ \t')
y = paste(x, collapse = '\n\n')
return(y)
}
main <- main %>%
sapply(., tidai) %>%
as.character() %>%
gsub("个股资料|操作策略|咨询高手|实盘买卖|基金持仓：,", "", .) %>%
gsub("(\\（|\\）)","\n==========\n", .)
all <- list(link, title, date, main)   ## all information: link, title, date, main
return(all)
}
inf = lapply(link[1], article)
inf
url <- "http://stock.stockstar.com/list/sectors.htm"
link <- url %>%
read_html(encoding="GB18030") %>%
html_nodes(".listtable  li  a") %>%
html_attr("href")
link
date <- url %>%
read_html(encoding="GB18030") %>%
html_nodes(".listtable .listnews  ul  span") %>%
html_text()
date
x=date[1]
substr(x,1,6)
date <- url %>%
read_html(encoding="GB18030") %>%
html_nodes(".listtable .listnews  ul  span") %>%
html_text() %>%
substr(., 1, 10)
date
which(date==Sys.Date())
date[which(date==Sys.Date())]
date <- date[which(date==Sys.Date())]
date <- url %>%
read_html(encoding="GB18030") %>%
html_nodes(".listtable .listnews  ul  span") %>%
html_text() %>%
substr(., 1, 10)  %>%
.[which(.==Sys.Date())]
date
link[1:length(date)]
browseURL(url)
url <- "http://stock.stockstar.com/list/sectors.htm"
date <- url %>%
read_html(encoding="GB18030") %>%
html_nodes(".listtable .listnews  ul  span") %>%
html_text() %>%
substr(., 1, 10)  %>%
.[which(.==Sys.Date())]
link <- url %>%
read_html(encoding="GB18030") %>%
html_nodes(".listtable  li  a") %>%
html_attr("href") %>%
.[1:length(date)]
url <- "http://stock.stockstar.com/list/sectors.htm"
link <- url %>%
read_html(encoding="GB18030") %>%
html_nodes(".listtable  li  a") %>%
html_attr("href")
link <- url %>%
read_html(encoding="GB18030") %>%
html_nodes(".listtable  li  a") %>%
html_attr("href") %>%
.[1:15]
link
