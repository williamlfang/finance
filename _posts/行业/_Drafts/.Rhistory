library(rvest)
library(downloader)
library(pipeR)
url <- "http://illuma.tmall.com/search.htm?spm=a1z10.4-b.w5001-7280993071.13.LhLkZK&scene=taobao_shop"
url %>% read_html() %>% html_nodes("h4 span") %>% html_text()
url %>% read_html() %>% html_nodes(".J_TItems h4 span") %>% html_text()
url %>% read_html() %>% html_nodes(".J_TItems .title") %>% html_text()
url %>% read_html() %>% html_nodes(".J_TItems .title") %>% html_text()
url %>% read_html() %>% html_nodes(".item4line1  h4  span") %>% html_text()
url %>% read_html() %>% html_nodes(".item4line1  h4") %>% html_text()
url <- "http://illuma.tmall.com/search.htm?spm=a1z10.4-b.w5001-7280993071.13.LhLkZK&scene=taobao_shop"
url %>% read_html() %>% html_nodes(".item4line1  h4") %>% html_text()
url
url %>% read_html() %>% html_nodes(".item4line1") %>% html_text()
library(rJava)
install.packages("rJava")
library(rJava)
install.packages("rJava")
install.packages("rJava")
install.packages("rJavax")
update.packages()
install.packages("rPython")
devtools::install_github("rstudio/rticles")
library(devtools)
library(devtool)
install.packages("devtools")
library(devtools)
library(devtools)
devtools::install_github("rstudio/rticles")
##  http://adv-r.had.co.nz/memory.html#garbarge-collection
mem <- function() {
bit <- 8L * .Machine$sizeof.pointer
if (!(bit == 32L || bit == 64L)) {
stop("Unknown architecture", call. = FALSE)
}
node_size <- if (bit == 32L) 28L else 56L
usage <- gc()
sum(usage[, 1] * c(node_size, 8)) / (1024 ^ 2)
}
## http://stackoverflow.com/questions/1358003/tricks-to-manage-the-available-memory-in-an-r-session
# improved list of objects
.ls.objects <- function (pos = 1, pattern, order.by,
decreasing=FALSE, head=FALSE, n=5) {
napply <- function(names, fn) sapply(names, function(x)
fn(get(x, pos = pos)))
names <- ls(pos = pos, pattern = pattern)
obj.class <- napply(names, function(x) as.character(class(x))[1])
obj.mode <- napply(names, mode)
obj.type <- ifelse(is.na(obj.class), obj.mode, obj.class)
obj.size <- napply(names, object.size)
obj.dim <- t(napply(names, function(x)
as.numeric(dim(x))[1:2]))
vec <- is.na(obj.dim)[, 1] & (obj.type != "function")
obj.dim[vec, 1] <- napply(names, length)[vec]
out <- data.frame(obj.type, obj.size, obj.dim)
names(out) <- c("Type", "Size", "Rows", "Columns")
if (!missing(order.by))
out <- out[order(out[[order.by]], decreasing=decreasing), ]
if (head)
out <- head(out, n)
out
}
# shorthand
lsos <- function(..., n=10) {
.ls.objects(..., order.by="Size", decreasing=TRUE, head=TRUE, n=n)
}
lsos
library(devtools)
install_github("williamlfang/rmarkdown")
library(devtools)
install_github("williamlfang/rmarkdown")
\end{figure}
x = "abc"
grep("a", x)
library(devtools)
install_github("williamlfang/rmarkdown")
install_github("rstudio/rmarkdown")
install_github("williamlfang/rmarkdown")
install_github("williamlfang/rmarkdown")
install_github("williamlfang/rmarkdown")
install_github("williamlfang/rmarkdown")
install_github("williamlfang/rmarkdown")
install_github("williamlfang/rmarkdown")
install_github("williamlfang/rmarkdown")
install_github("williamlfang/rmarkdown")
install_github("williamlfang/rmarkdown")
install_github("williamlfang/rmarkdown")
install_github("williamlfang/rmarkdown")
50 000 * 28  + 800 000 + 45 000 + 50 000 * 8
50000 * 28  + 800000 + 45000 + 50000 * 8
50000 * 28  + 800000 + 45000 + 50000 * 8
options(digits=8)
help(digits)
??digits
help(options)
options(digits=1)
50000 * 28  + 800000 + 45000 + 50000 * 8
1310000 + 1382530 + 910000 + 2270000 + 2070000 + 1623270 + 1110000
option(digits=12)
options(digits=12)
1310000 + 1382530 + 910000 + 2270000 + 2070000 + 1623270 + 1110000
1310000 + 1382530 + 910000 + 2270000 + 2070000 + 1623270 + 1110000 + 19359420
library(rvest)
library(downloader)
library(beepr)
###########################################################################
days <- 6
###########################################################################
## download function
dl <- function(x){
destfile <- paste(x[[1]], "-", x[[2]], ".pdf", sep="")
try(download(x[[3]], destfile))
}
## download function
dl2 <- function(x){
destfile <- paste(x[[1]][1], "-", x[[2]][1], ".pdf", sep="")
if (file.exists(destfile)){
print("File Already Existed!")
} else {
# try(download(all[[4]][1], destfile))
try(download(x[[3]][1], destfile))
}
}
url <- rep(NA)
for(i in 1:(days*4) ){
url[i] <- paste0("http://stock.stockstar.com/list/3491_",i,".shtml")
}
url
url <- rep(NA)
for(i in 1:(days*4) ){
url[i] <- paste0("http://stock.stockstar.com/list/3491_",i,".shtml")
}
linkings <- function(x){
x %>%
read_html(encoding="GB18030") %>%
html_nodes(".listtable  li  a") %>%
html_attr("href")
}
links <- lapply(url, linkings)
links
datings <- function(x){
x %>%
read_html(encoding="GB18030") %>%
html_nodes(".listnews   span") %>%
html_text() %>%
substr(., 1, 10)
}
dates <- lapply(url, datings)
dates <- lapply(url[1], datings)
dates
dates <- lapply(url, datings)
dates
links
url
require(knitr)
names(knitr_engines$get())
names(knitr_engines$get())
names(knit_engines$get())
\end{comment}
begining <- which(TD==date[1])
######################################################################################################
rm(list=ls())
######################################################################################################
library(rvest)
library(downloader)
######################################################################################################
######################################################################################################
setwd("/home/william/Desktop/df")
files <- list.files(pattern=".html", ignore.case=TRUE)
linkings <- function(x){
x %>%
read_html(encoding="UTF-8") %>%
html_nodes("a") %>%
html_attr("href")
}
comp <- lapply(files[grep("ind", files)], linkings)   ## comp
com <- list(NA)
company <- rep(NA)
for(i in 1:length(comp)){
com[[i]] <- comp[[i]][grep(".+Industry.html$", comp[[i]])]
for(j in 1:50){
company[(i-1)*50 + j] <- com[[i]][j]
}
}
datings <- function(x){
report_infos <- x %>%
read_html(encoding="GB18030") %>%
html_nodes(".report-infos span") %>%
html_text() %>%
gsub("\r\n +", "", .)
date <- report_infos %>% .[grep("2015", .)] %>% strsplit(., "年|月|日") %>% .[[1]]
date <- paste(date[1], date[2], date[3], sep="-")
return(date)
}
date <- rep(NA)
for(i in 1:length(company)){
date[i] <- try(datings(company[i]))
cat("-------------", i, "of", length(company), "-------------\n")
}
date <- sort(date)
RPP_comp <- function(x){
subtitle  <- x %>%
read_html(encoding="GB18030") %>%
html_nodes("h1") %>%
html_text() %>%
gsub("\r\n +", "", .)
report_name <- x %>%
read_html(encoding="GB18030") %>%
html_nodes(".titlebar  .tit  a") %>%
html_text() %>%
.[1] %>%
gsub("研究报告", "", .)
##  ticker  <- x %>%
##    read_html(encoding="GB18030") %>%
##    html_nodes(".report-content  p") %>%
##    html_text() %>%
##    .[2]
##  title <- paste(ticker, subtitle, sep="：")
title <- paste(report_name, subtitle, sep="：")
report_infos <- x %>%
read_html(encoding="GB18030") %>%
html_nodes(".report-infos span") %>%
html_text() %>%
gsub("\r\n +", "", .)
date <- report_infos %>% .[grep("201", .)] %>% strsplit(., "年|月|日") %>% .[[1]]
date <- paste(date[1], date[2], date[3], sep="-")
file.link <-  x %>%
read_html(encoding="GB18030") %>%
html_nodes(".report-content  a") %>%
html_attr("href") %>%
.[ grep("\\.pdf$", .) ] %>%
.[1]
all <- list(title, date, file.link)   ## all information: link, title, date, main
destfile <- paste0(all[[1]][[1]], "-", all[[2]][1], ".pdf")
if (file.exists(destfile)){
print("File Already Existed!")
} else {
# try(download(all[[4]][1], destfile))
try(download(all[[3]][1], destfile))
}
}
#######################################################################333
setwd("/media/william/Storage/国信证券/Reports/东方财富网")
TD<- read.csv("TradingDates.csv", header=TRUE, stringsAsFactors=F)
TD <- TD[-166,1]
###############################################################################
TD
beginDate <- which(TD==date[1])
endDate <- which(TD==date[length(date)])
date
Date <- TD[167]
Date
which(date==Date)[1]
(which(date==Date)[1]+length(which(date==Date))-1)
######################################################################################################
rm(list=ls())
######################################################################################################
library(rvest)
library(downloader)
######################################################################################################
#days <- 6
######################################################################################################
setwd("/home/william/Desktop/df")
files <- list.files(pattern=".html", ignore.case=TRUE)
linkings <- function(x){
x %>%
read_html(encoding="UTF-8") %>%
html_nodes("a") %>%
html_attr("href")
}
comp <- lapply(files[grep("comp", files)], linkings)   ## comp
com <- list(NA)
company <- rep(NA)
for(i in 1:length(comp)){
com[[i]] <- comp[[i]][grep(".+Report.html$", comp[[i]])]
for(j in 1:50){
company[(i-1)*50 + j] <- com[[i]][j]
}
}
datings <- function(x){
report_infos <- x %>%
read_html(encoding="GB18030") %>%
html_nodes(".report-infos span") %>%
html_text() %>%
gsub("\r\n +", "", .)
date <- report_infos %>% .[grep("2015", .)] %>% strsplit(., "年|月|日") %>% .[[1]]
date <- paste(date[1], date[2], date[3], sep="-")
return(date)
}
#dates <- lapply(company, datings)
# dates <-  lapply(company [1:5],datings)
#date <- rep(NA)
#for(i in 1:length(dates)){
#  date[i] <- dates[[i]][1]
#}
date <- rep(NA)
for(i in 1:length(company)){
date[i] <- try(datings(company[i]))
cat("-------------", i, "of", length(company), "-------------\n")
}
date <- sort(date)
date
RPP_comp <- function(x){
subtitle  <- x %>%
read_html(encoding="GB18030") %>%
html_nodes("h1") %>%
html_text() %>%
gsub("\r\n +", "", .)
report_name <- x %>%
read_html(encoding="GB18030") %>%
html_nodes(".titlebar  .tit  a") %>%
html_text() %>%
.[1] %>%
gsub("研究报告", "", .)
##  ticker  <- x %>%
##    read_html(encoding="GB18030") %>%
##    html_nodes(".report-content  p") %>%
##    html_text() %>%
##    .[2]
##  title <- paste(ticker, subtitle, sep="：")
title <- paste(report_name, subtitle, sep="：")
report_infos <- x %>%
read_html(encoding="GB18030") %>%
html_nodes(".report-infos span") %>%
html_text() %>%
gsub("\r\n +", "", .)
date <- report_infos %>% .[grep("201", .)] %>% strsplit(., "年|月|日") %>% .[[1]]
date <- paste(date[1], date[2], date[3], sep="-")
file.link <-  x %>%
read_html(encoding="GB18030") %>%
html_nodes(".report-content  a") %>%
html_attr("href") %>%
.[ grep("\\.pdf$", .) ] %>%
.[1]
all <- list(title, date, file.link)   ## all information: link, title, date, main
destfile <- paste0(all[[1]][[1]], "-", all[[2]][1], ".pdf")
if (file.exists(destfile)){
print("File Already Existed!")
} else {
# try(download(all[[4]][1], destfile))
try(download(all[[3]][1], destfile))
}
}
#######################################################################333
setwd("/media/william/Storage/国信证券/Reports/东方财富网")
TD<- read.csv("TradingDates.csv", header=TRUE, stringsAsFactors=F)
TD <- TD[-166,1]
###############################################################################
TD
beginDate <- which(TD==date[1])
endDate <- which(TD==date[length(date)])
beginDate
endDate
source('~/finance/_posts/行业/_Drafts/industry (copy).R', echo=TRUE)
url <- "http://stock.stockstar.com/list/sectors.htm"
link <- url %>%
read_html(encoding="GB18030") %>%
html_nodes(".listtable  li  a") %>%
html_attr("href") %>%
.[1:10]
article <- function(x){
link <- x
title  <- x %>%
read_html(encoding="GB18030") %>%
html_nodes("h1") %>%
html_text()
page.numbers <- x %>%
read_html(encoding="GB18030")  %>%
html_nodes("#Page a") %>%
html_text() %>%
as.numeric()
page.numbers <- page.numbers[1:length(page.numbers)-1]
date <- x %>%
read_html(encoding="GB18030") %>%
html_nodes("#pubtime_baidu") %>%
html_text()
url0 <- strsplit(x, "\\.shtml")
url.new <- rep( NA, length(page.numbers) )
for ( i in 1:length(page.numbers) ){
url.new[i] <- paste(url0, "_", i, ".shtml", sep="")
}
text <- function(x){
text <- x %>%
read_html(encoding="GB18030")  %>%
html_nodes("#container-article  p") %>%
html_text()
text <- text[3:length(text)-1]
}
main <- lapply(url.new, text)
tidai <- function(x){
# y = paste(x, collapse = '\t ============================================================================ \t')
y = paste(x, collapse = '\n\n')
return(y)
}
main <- main %>%
sapply(., tidai) %>%
as.character() %>%
gsub("个股资料|操作策略|咨询高手|实盘买卖|基金持仓：,", "", .) %>%
gsub("(\\（|\\）)","\n==========\n", .)
all <- list(link, title, date, main)   ## all information: link, title, date, main
return(all)
}
inf = lapply(link, article)
link
inf = lapply(link[1:10], article)
x = "http://stock.stockstar.com/SS2015092800001483_1.shtml"
inf = lapply(x, article)
link
inf = lapply(link[1:5], article)
inf = lapply(link[1:3], article)
inf
inf = lapply(link[4], article)
inf = lapply(link[5], article)
inf <- list(NA)
inf
i=1
inf[[i]] < article(link[i])
inf
x = article
x < article(link[i])
inf[[i]] < lapply(link[i], article)
inf <- list(NA)
inf[i]
inf[i] < lapply(link[i], article)
inf = lapply(link[1], article)
str(inf)
inf <- list(list(NA))
inf[i] < lapply(link[i], article)
inf[[i]] < lapply(link[i], article)
inf = lapply(link[1], article)
inf
inf[[1]][[2]]
inf[[1]][[3]]
inf[[1]][[1]]
inf[[1]][[4]]
link[1:10]
inf = lapply(link[1], article)
inf = lapply(link[10], article)
inf[[1]][[3]]
inf[[1]][[1]]
inf[[1]][[4]]
inf = lapply(link[9], article)
inf = lapply(link[7], article)
link[7]
browseURL(link[7])
browseURL(link[6])
x =link[7]
link <- x
title  <- x %>%
read_html(encoding="GB18030") %>%
html_nodes("h1") %>%
html_text()
title
page.numbers <- x %>%
read_html(encoding="GB18030")  %>%
html_nodes("#Page a") %>%
html_text() %>%
as.numeric()
page.numbers <- page.numbers[1:length(page.numbers)-1]
page.numbers
date <- x %>%
read_html(encoding="GB18030") %>%
html_nodes("#pubtime_baidu") %>%
html_text()
url0 <- strsplit(x, "\\.shtml")
url.new <- rep( NA, length(page.numbers) )
for ( i in 1:length(page.numbers) ){
url.new[i] <- paste(url0, "_", i, ".shtml", sep="")
}
url.new
text <- function(x){
text <- x %>%
read_html(encoding="GB18030")  %>%
html_nodes("#container-article  p") %>%
html_text()
text <- text[3:length(text)-1]
}
main <- lapply(url.new, text)
url.new
lapply(url.new[1], text)
lapply(url.new[6], text)
lapply(url.new[5], text)
lapply(url.new, text)
main <- lapply(url.new, text)
tidai <- function(x){
# y = paste(x, collapse = '\t ============================================================================ \t')
y = paste(x, collapse = '\n\n')
return(y)
}
main <- main %>%
sapply(., tidai) %>%
as.character() %>%
gsub("个股资料|操作策略|咨询高手|实盘买卖|基金持仓：,", "", .) %>%
gsub("(\\（|\\）)","\n==========\n", .)
all <- list(link, title, date, main)   ## all information: link, title, date, main
all
inf = lapply(link[7], article)
link[7]
link
url <- "http://stock.stockstar.com/list/sectors.htm"
link <- url %>%
read_html(encoding="GB18030") %>%
html_nodes(".listtable  li  a") %>%
html_attr("href") %>%
.[1:10]
inf = lapply(link[7], article)
source('~/finance/_posts/行业/_Drafts/industry (copy).R', echo=TRUE)
inf = lapply(link[5], article)
file.new <- paste0(Sys.Date(), "-证券之星.md")
to.file = paste0("~/finance/_posts/行业/", file.new)
file.copy("证券之星2.md", to = to.file,  overwrite = TRUE)
