inf = lapply(link, article)
##:
## inf[[]][[1]]: title
## inf[[]][[2]]: date
## inf[[]][[3]]: file.link
lapply(inf, dl)
################### Page 4 ###################
## Report Paper Links
link <- url[4] %>%
read_html(encoding="GB18030") %>%
html_nodes(".newslist_content  li  a") %>%
html_attr("href")
inf = lapply(link, article)
##:
## inf[[]][[1]]: title
## inf[[]][[2]]: date
## inf[[]][[3]]: file.link
lapply(inf, dl)
################### Page 5 ###################
## Report Paper Links
link <- url[5] %>%
read_html(encoding="GB18030") %>%
html_nodes(".newslist_content  li  a") %>%
html_attr("href")
inf = lapply(link, article)
##:
## inf[[]][[1]]: title
## inf[[]][[2]]: date
## inf[[]][[3]]: file.link
lapply(inf, dl)
}
else{
url <- c(paste("http://news.stockstar.com/info/dstock.aspx?PageId=1&id=3491&code=", TickerCode, sep=""),
paste("http://news.stockstar.com/info/dstock.aspx?PageId=2&id=3491&code=", TickerCode, sep=""),
paste("http://news.stockstar.com/info/dstock.aspx?PageId=3&id=3491&code=", TickerCode, sep=""),
paste("http://news.stockstar.com/info/dstock.aspx?PageId=4&id=3491&code=", TickerCode, sep=""),
paste("http://news.stockstar.com/info/dstock.aspx?PageId=5&id=3491&code=", TickerCode, sep=""),
paste("http://news.stockstar.com/info/dstock.aspx?PageId=6&id=3491&code=", TickerCode, sep=""))
##
## Report Paper Links
link <- url[1] %>%
read_html(encoding="GB18030") %>%
html_nodes(".newslist_content  li  a") %>%
html_attr("href")
inf = lapply(link, article)
##:
## inf[[]][[1]]: title
## inf[[]][[2]]: date
## inf[[]][[3]]: file.link
lapply(inf, dl)
################### Page 2 ###################
## Report Paper Links
link <- url[2] %>%
read_html(encoding="GB18030") %>%
html_nodes(".newslist_content  li  a") %>%
html_attr("href")
inf = lapply(link, article)
##:
## inf[[]][[1]]: title
## inf[[]][[2]]: date
## inf[[]][[3]]: file.link
lapply(inf, dl)
################### Page 3 ###################
## Report Paper Links
link <- url[3] %>%
read_html(encoding="GB18030") %>%
html_nodes(".newslist_content  li  a") %>%
html_attr("href")
inf = lapply(link, article)
##:
## inf[[]][[1]]: title
## inf[[]][[2]]: date
## inf[[]][[3]]: file.link
lapply(inf, dl)
################### Page 4 ###################
## Report Paper Links
link <- url[4] %>%
read_html(encoding="GB18030") %>%
html_nodes(".newslist_content  li  a") %>%
html_attr("href")
inf = lapply(link, article)
##:
## inf[[]][[1]]: title
## inf[[]][[2]]: date
## inf[[]][[3]]: file.link
lapply(inf, dl)
################### Page 5 ###################
## Report Paper Links
link <- url[5] %>%
read_html(encoding="GB18030") %>%
html_nodes(".newslist_content  li  a") %>%
html_attr("href")
inf = lapply(link, article)
##:
## inf[[]][[1]]: title
## inf[[]][[2]]: date
## inf[[]][[3]]: file.link
lapply(inf, dl)
################### Page 6 ###################
## Report Paper Links
link <- url[6] %>%
read_html(encoding="GB18030") %>%
html_nodes(".newslist_content  li  a") %>%
html_attr("href")
inf = lapply(link, article)
##:
## inf[[]][[1]]: title
## inf[[]][[2]]: date
## inf[[]][[3]]: file.link
lapply(inf, dl)
}
}
}
}
}
beep()
}
require(knitr, quietly=TRUE, warn.conflicts=FALSE)
require(rmarkdown)
library(rvest)
c
setwd("~/finance/_posts/资讯")
dir=getwd()
images.dir=dir
images.url='/investment/assets/images/'
out_ext='.md'
in_ext='.Rmd'
recursive=FALSE
files <- list.files(path=dir, pattern=in_ext, ignore.case=TRUE, recursive=recursive) %>%
.[which(substr(.,1, 10)==Sys.Date())]               ## Sys.Date()
files
files
for(f in files) {
message(paste("Processing ", f, sep=''))
content <- readLines(f)
frontMatter <- which(substr(content, 1, 3) == '---')
if(length(frontMatter) >= 2 & 1 %in% frontMatter) {
statusLine <- which(substr(content, 1, 7) == 'status:')
publishedLine <- which(substr(content, 1, 10) == 'published:')
if(statusLine > frontMatter[1] & statusLine < frontMatter[2]) {
status <- unlist(strsplit(content[statusLine], ':'))[2]
status <- sub('[[:space:]]+$', '', status)
status <- sub('^[[:space:]]+', '', status)
if(tolower(status) == 'process') {
#This is a bit of a hack but if a line has zero length (i.e. a
#black line), it will be removed in the resulting markdown file.
#This will ensure that all line returns are retained.
content[nchar(content) == 0] <- ' '
message(paste('Processing ', f, sep=''))
content[statusLine] <- 'status: publish'
content[publishedLine] <- 'published: true'
outFile <- paste(substr(f, 1, (nchar(f)-(nchar(in_ext)))), out_ext, sep='')
render_markdown(strict=TRUE)
opts_knit$set(out.format='md')
opts_knit$set(base.dir=images.dir)
opts_knit$set(base.url=images.url)
######################################################################
## 产生的图片存储位置 `/assets/images/r-figures/`
fig.path <- paste0("r-figures/", sub(".Rmd$", "", basename(files)), "/")
opts_chunk$set(fig.path = fig.path)
## opts_chunk$set(fig.cap = "center")  ## figure position
## render_jekyll()
######################################################################
try(knit(text=content, output=outFile), silent=FALSE)
} else {
warning(paste("Not processing ", f, ", status is '", status,
"'. Set status to 'process' to convert.", sep=''))
}
} else {
warning("Status not found in front matter.")
}
} else {
warning("No front matter found. Will not process this file.")
}
library(beepr);beep()
}
files
f=files[1]
f
message(paste("Processing ", f, sep=''))
content <- readLines(f)
frontMatter <- which(substr(content, 1, 3) == '---')
if(length(frontMatter) >= 2 & 1 %in% frontMatter) {
statusLine <- which(substr(content, 1, 7) == 'status:')
publishedLine <- which(substr(content, 1, 10) == 'published:')
if(statusLine > frontMatter[1] & statusLine < frontMatter[2]) {
status <- unlist(strsplit(content[statusLine], ':'))[2]
status <- sub('[[:space:]]+$', '', status)
status <- sub('^[[:space:]]+', '', status)
if(tolower(status) == 'process') {
#This is a bit of a hack but if a line has zero length (i.e. a
#black line), it will be removed in the resulting markdown file.
#This will ensure that all line returns are retained.
content[nchar(content) == 0] <- ' '
message(paste('Processing ', f, sep=''))
content[statusLine] <- 'status: publish'
content[publishedLine] <- 'published: true'
outFile <- paste(substr(f, 1, (nchar(f)-(nchar(in_ext)))), out_ext, sep='')
render_markdown(strict=TRUE)
opts_knit$set(out.format='md')
opts_knit$set(base.dir=images.dir)
opts_knit$set(base.url=images.url)
######################################################################
## 产生的图片存储位置 `/assets/images/r-figures/`
fig.path <- paste0("r-figures/", sub(".Rmd$", "", basename(files)), "/")
opts_chunk$set(fig.path = fig.path)
## opts_chunk$set(fig.cap = "center")  ## figure position
## render_jekyll()
######################################################################
try(knit(text=content, output=outFile), silent=FALSE)
} else {
warning(paste("Not processing ", f, ", status is '", status,
"'. Set status to 'process' to convert.", sep=''))
}
} else {
warning("Status not found in front matter.")
}
} else {
warning("No front matter found. Will not process this file.")
}
library(beepr);beep()
f=files[2]
f
f=files[3]
f
message(paste("Processing ", f, sep=''))
content <- readLines(f)
frontMatter <- which(substr(content, 1, 3) == '---')
if(length(frontMatter) >= 2 & 1 %in% frontMatter) {
statusLine <- which(substr(content, 1, 7) == 'status:')
publishedLine <- which(substr(content, 1, 10) == 'published:')
if(statusLine > frontMatter[1] & statusLine < frontMatter[2]) {
status <- unlist(strsplit(content[statusLine], ':'))[2]
status <- sub('[[:space:]]+$', '', status)
status <- sub('^[[:space:]]+', '', status)
if(tolower(status) == 'process') {
#This is a bit of a hack but if a line has zero length (i.e. a
#black line), it will be removed in the resulting markdown file.
#This will ensure that all line returns are retained.
content[nchar(content) == 0] <- ' '
message(paste('Processing ', f, sep=''))
content[statusLine] <- 'status: publish'
content[publishedLine] <- 'published: true'
outFile <- paste(substr(f, 1, (nchar(f)-(nchar(in_ext)))), out_ext, sep='')
render_markdown(strict=TRUE)
opts_knit$set(out.format='md')
opts_knit$set(base.dir=images.dir)
opts_knit$set(base.url=images.url)
######################################################################
## 产生的图片存储位置 `/assets/images/r-figures/`
fig.path <- paste0("r-figures/", sub(".Rmd$", "", basename(files)), "/")
opts_chunk$set(fig.path = fig.path)
## opts_chunk$set(fig.cap = "center")  ## figure position
## render_jekyll()
######################################################################
try(knit(text=content, output=outFile), silent=FALSE)
} else {
warning(paste("Not processing ", f, ", status is '", status,
"'. Set status to 'process' to convert.", sep=''))
}
} else {
warning("Status not found in front matter.")
}
} else {
warning("No front matter found. Will not process this file.")
}
library(beepr);beep()
require(knitr, quietly=TRUE, warn.conflicts=FALSE)
require(rmarkdown)
library(rvest)
setwd("~/finance/_posts/公司")
setwd("~/finance/_posts/行业")
dir=getwd()
images.dir=dir
images.url='/investment/assets/images/'
out_ext='.md'
in_ext='.Rmd'
recursive=FALSE
files <- list.files(path=dir, pattern=in_ext, ignore.case=TRUE, recursive=recursive) %>%
.[which(substr(.,1, 10)==Sys.Date())]               ## Sys.Date()
files
f = files[length(files)]                  ################ wich files ##############
message(paste("Processing ", f, sep=''))
content <- readLines(f)
frontMatter <- which(substr(content, 1, 3) == '---')
if(length(frontMatter) >= 2 & 1 %in% frontMatter) {
statusLine <- which(substr(content, 1, 7) == 'status:')
publishedLine <- which(substr(content, 1, 10) == 'published:')
if(statusLine > frontMatter[1] & statusLine < frontMatter[2]) {
status <- unlist(strsplit(content[statusLine], ':'))[2]
status <- sub('[[:space:]]+$', '', status)
status <- sub('^[[:space:]]+', '', status)
if(tolower(status) == 'process') {
#This is a bit of a hack but if a line has zero length (i.e. a
#black line), it will be removed in the resulting markdown file.
#This will ensure that all line returns are retained.
content[nchar(content) == 0] <- ' '
message(paste('Processing ', f, sep=''))
content[statusLine] <- 'status: publish'
content[publishedLine] <- 'published: true'
outFile <- paste(substr(f, 1, (nchar(f)-(nchar(in_ext)))), out_ext, sep='')
render_markdown(strict=TRUE)
opts_knit$set(out.format='md')
opts_knit$set(base.dir=images.dir)
opts_knit$set(base.url=images.url)
######################################################################
## 产生的图片存储位置 `/assets/images/r-figures/`
fig.path <- paste0("r-figures/", sub(".Rmd$", "", basename(files)), "/")
opts_chunk$set(fig.path = fig.path)
## opts_chunk$set(fig.cap = "center")  ## figure position
## render_jekyll()
######################################################################
try(knit(text=content, output=outFile), silent=FALSE)
} else {
warning(paste("Not processing ", f, ", status is '", status,
"'. Set status to 'process' to convert.", sep=''))
}
} else {
warning("Status not found in front matter.")
}
} else {
warning("No front matter found. Will not process this file.")
}
library(beepr);beep()
f
message(paste("Processing ", f, sep=''))
content <- readLines(f)
frontMatter <- which(substr(content, 1, 3) == '---')
if(length(frontMatter) >= 2 & 1 %in% frontMatter) {
statusLine <- which(substr(content, 1, 7) == 'status:')
publishedLine <- which(substr(content, 1, 10) == 'published:')
if(statusLine > frontMatter[1] & statusLine < frontMatter[2]) {
status <- unlist(strsplit(content[statusLine], ':'))[2]
status <- sub('[[:space:]]+$', '', status)
status <- sub('^[[:space:]]+', '', status)
if(tolower(status) == 'process') {
#This is a bit of a hack but if a line has zero length (i.e. a
#black line), it will be removed in the resulting markdown file.
#This will ensure that all line returns are retained.
content[nchar(content) == 0] <- ' '
message(paste('Processing ', f, sep=''))
content[statusLine] <- 'status: publish'
content[publishedLine] <- 'published: true'
outFile <- paste(substr(f, 1, (nchar(f)-(nchar(in_ext)))), out_ext, sep='')
render_markdown(strict=TRUE)
opts_knit$set(out.format='md')
opts_knit$set(base.dir=images.dir)
opts_knit$set(base.url=images.url)
######################################################################
## 产生的图片存储位置 `/assets/images/r-figures/`
fig.path <- paste0("r-figures/", sub(".Rmd$", "", basename(files)), "/")
opts_chunk$set(fig.path = fig.path)
## opts_chunk$set(fig.cap = "center")  ## figure position
## render_jekyll()
######################################################################
try(knit(text=content, output=outFile), silent=FALSE)
} else {
warning(paste("Not processing ", f, ", status is '", status,
"'. Set status to 'process' to convert.", sep=''))
}
} else {
warning("Status not found in front matter.")
}
} else {
warning("No front matter found. Will not process this file.")
}
library(beepr);beep()
library(rvest)
url <- "http://stock.stockstar.com/list/sectors.htm"
link <- url %>%
read_html(encoding="GB18030") %>%
html_nodes(".listtable  li  a") %>%
html_attr("href")
article <- function(x){
link <- x
title  <- x %>%
read_html(encoding="GB18030") %>%
html_nodes("h1") %>%
html_text()
page.numbers <- x %>%
read_html(encoding="GB18030")  %>%
html_nodes("#Page a") %>%
html_text() %>%
as.numeric()
page.numbers <- page.numbers[1:length(page.numbers)-1]
date <- x %>%
read_html(encoding="GB18030") %>%
html_nodes("#pubtime_baidu") %>%
html_text()
url0 <- strsplit(x, "\\.shtml")
url.new <- rep( NA, length(page.numbers) )
for ( i in 1:length(page.numbers) ){
url.new[i] <- paste(url0, "_", i, ".shtml", sep="")
}
text <- function(x){
text <- x %>%
read_html(encoding="GB18030")  %>%
html_nodes("#container-article  p") %>%
html_text()
text <- text[3:length(text)-1]
}
main <- lapply(url.new, text)
tidai <- function(x){
# y = paste(x, collapse = '\t ============================================================================ \t')
y = paste(x, collapse = '\n\n')
return(y)
}
main <- main %>%
sapply(., tidai) %>%
as.character() %>%
gsub("个股资料|操作策略|咨询高手|实盘买卖|基金持仓：,", "", .) %>%
gsub("(\\（|\\）)","\n==========\n", .)
all <- list(link, title, date, main)   ## all information: link, title, date, main
return(all)
}
inf = lapply(link, article)
url <- "http://stock.stockstar.com/list/sectors.htm"
link <- url %>%
read_html(encoding="GB18030") %>%
html_nodes(".listtable  li  a") %>%
html_attr("href")
article <- function(x){
link <- x
title  <- x %>%
read_html(encoding="GB18030") %>%
html_nodes("h1") %>%
html_text()
page.numbers <- x %>%
read_html(encoding="GB18030")  %>%
html_nodes("#Page a") %>%
html_text() %>%
as.numeric()
page.numbers <- page.numbers[1:length(page.numbers)-1]
date <- x %>%
read_html(encoding="GB18030") %>%
html_nodes("#pubtime_baidu") %>%
html_text()
url0 <- strsplit(x, "\\.shtml")
url.new <- rep( NA, length(page.numbers) )
for ( i in 1:length(page.numbers) ){
url.new[i] <- paste(url0, "_", i, ".shtml", sep="")
}
text <- function(x){
text <- x %>%
read_html(encoding="GB18030")  %>%
html_nodes("#container-article  p") %>%
html_text()
text <- text[3:length(text)-1]
}
main <- lapply(url.new, text)
tidai <- function(x){
# y = paste(x, collapse = '\t ============================================================================ \t')
y = paste(x, collapse = '\n\n')
return(y)
}
main <- main %>%
sapply(., tidai) %>%
as.character() %>%
gsub("个股资料|操作策略|咨询高手|实盘买卖|基金持仓：,", "", .) %>%
gsub("(\\（|\\）)","\n==========\n", .)
all <- list(link, title, date, main)   ## all information: link, title, date, main
return(all)
}
inf = lapply(link, article)
url <- "http://stock.stockstar.com/list/sectors.htm"
link <- url %>%
read_html(encoding="GB18030") %>%
html_nodes(".listtable  li  a") %>%
html_attr("href")
article <- function(x){
link <- x
title  <- x %>%
read_html(encoding="GB18030") %>%
html_nodes("h1") %>%
html_text()
page.numbers <- x %>%
read_html(encoding="GB18030")  %>%
html_nodes("#Page a") %>%
html_text() %>%
as.numeric()
page.numbers <- page.numbers[1:length(page.numbers)-1]
date <- x %>%
read_html(encoding="GB18030") %>%
html_nodes("#pubtime_baidu") %>%
html_text()
url0 <- strsplit(x, "\\.shtml")
url.new <- rep( NA, length(page.numbers) )
for ( i in 1:length(page.numbers) ){
url.new[i] <- paste(url0, "_", i, ".shtml", sep="")
}
text <- function(x){
text <- x %>%
read_html(encoding="GB18030")  %>%
html_nodes("#container-article  p") %>%
html_text()
text <- text[3:length(text)-1]
}
main <- lapply(url.new, text)
tidai <- function(x){
# y = paste(x, collapse = '\t ============================================================================ \t')
y = paste(x, collapse = '\n\n')
return(y)
}
main <- main %>%
sapply(., tidai) %>%
as.character() %>%
gsub("个股资料|操作策略|咨询高手|实盘买卖|基金持仓：,", "", .) %>%
gsub("(\\（|\\）)","\n==========\n", .)
all <- list(link, title, date, main)   ## all information: link, title, date, main
return(all)
}
inf = lapply(link, article)
